{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Curricular units 2nd sem (approved)',\n",
    " 'Admission grade',\n",
    " 'Curricular units 1st sem (grade)',\n",
    " 'Tuition fees up to date',\n",
    " 'Curricular units 1st sem (enrolled)',\n",
    " 'Curricular units 1st sem (evaluations)',  \n",
    " 'Course',\n",
    " 'Previous qualification (grade)',\n",
    " \"Mother's occupation\",\n",
    " 'Age at enrollment',\n",
    " 'GDP',\n",
    " 'Unemployment rate',\n",
    " 'Inflation rate',\n",
    " 'Curricular units 2nd sem (without evaluations)',\n",
    " 'Nacionality']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: ['Curricular units 2nd sem (approved)', 'Admission grade', 'Curricular units 1st sem (grade)', 'Tuition fees up to date', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Course', 'Previous qualification (grade)', \"Mother's occupation\", 'Age at enrollment', 'GDP', 'Unemployment rate', 'Inflation rate', 'Curricular units 2nd sem (without evaluations)', 'Nacionality']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with the correct delimiter (semicolon)\n",
    "dropout_data = pd.read_csv(\"dropout.csv\", delimiter=';')\n",
    "\n",
    "# Convert the 'Target' column into numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "dropout_data['Target'] = label_encoder.fit_transform(dropout_data['Target'])\n",
    "\n",
    "# Split the data into features and the target\n",
    "X = dropout_data.drop(columns=['Target'])\n",
    "X = X[['Curricular units 2nd sem (approved)',\n",
    " 'Admission grade',\n",
    " 'Curricular units 1st sem (grade)',\n",
    " 'Tuition fees up to date',\n",
    " 'Curricular units 1st sem (enrolled)',\n",
    " 'Curricular units 1st sem (evaluations)',\n",
    " 'Course',\n",
    " 'Previous qualification (grade)',\n",
    " \"Mother's occupation\",\n",
    " 'Age at enrollment',\n",
    " 'GDP',\n",
    " 'Unemployment rate',\n",
    " 'Inflation rate',\n",
    " 'Curricular units 2nd sem (without evaluations)',\n",
    " 'Nacionality']]\n",
    "print(f\"Feature: {list(X.columns)}\")\n",
    "# X = X[outputs[0]]\n",
    "y = dropout_data['Target']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LALA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(random_state=0)\n",
    "rfc = RandomForestClassifier(random_state=0)\n",
    "lr = LogisticRegression(random_state=40)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "abc = AdaBoostClassifier(n_estimators=50,learning_rate=1, random_state=0)\n",
    "xbc = XGBClassifier(tree_method='gpu_hist')\n",
    "svc = svm.SVC(kernel='linear',probability=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LALA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='linear', probability=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtree.fit(X_train,y_train)\n",
    "rfc.fit(X_train,y_train)\n",
    "lr.fit(X_train,y_train)\n",
    "knn.fit(X_train,y_train)\n",
    "abc.fit(X_train, y_train)\n",
    "xbc.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Accuracy\t\t : 68.13559 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.66      0.69      0.67       284\n",
      "    Enrolled       0.34      0.35      0.35       151\n",
      "    Graduate       0.81      0.79      0.80       450\n",
      "\n",
      "    accuracy                           0.68       885\n",
      "   macro avg       0.61      0.61      0.61       885\n",
      "weighted avg       0.68      0.68      0.68       885\n",
      "\n",
      "RandomForest Accuracy\t\t : 78.19209 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.79      0.76      0.77       284\n",
      "    Enrolled       0.58      0.34      0.43       151\n",
      "    Graduate       0.81      0.95      0.87       450\n",
      "\n",
      "    accuracy                           0.78       885\n",
      "   macro avg       0.73      0.68      0.69       885\n",
      "weighted avg       0.77      0.78      0.77       885\n",
      "\n",
      "LogisticRegression Accuracy\t : 76.27119 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.78      0.77      0.77       284\n",
      "    Enrolled       0.52      0.21      0.30       151\n",
      "    Graduate       0.78      0.94      0.85       450\n",
      "\n",
      "    accuracy                           0.76       885\n",
      "   macro avg       0.70      0.64      0.64       885\n",
      "weighted avg       0.74      0.76      0.73       885\n",
      "\n",
      "KNeighbors Accuracy\t\t : 70.84746 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.68      0.74      0.71       284\n",
      "    Enrolled       0.38      0.28      0.33       151\n",
      "    Graduate       0.81      0.83      0.82       450\n",
      "\n",
      "    accuracy                           0.71       885\n",
      "   macro avg       0.62      0.62      0.62       885\n",
      "weighted avg       0.69      0.71      0.70       885\n",
      "\n",
      "AdaBoost Accuracy\t\t : 76.0452 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.78      0.77      0.78       284\n",
      "    Enrolled       0.48      0.33      0.39       151\n",
      "    Graduate       0.81      0.90      0.85       450\n",
      "\n",
      "    accuracy                           0.76       885\n",
      "   macro avg       0.69      0.67      0.67       885\n",
      "weighted avg       0.74      0.76      0.75       885\n",
      "\n",
      "XGBoost Accuracy\t\t : 76.0452 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.78      0.75      0.76       284\n",
      "    Enrolled       0.47      0.41      0.44       151\n",
      "    Graduate       0.83      0.88      0.86       450\n",
      "\n",
      "    accuracy                           0.76       885\n",
      "   macro avg       0.69      0.68      0.69       885\n",
      "weighted avg       0.75      0.76      0.76       885\n",
      "\n",
      "SVM Accuracy\t\t\t : 76.38418 %\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Dropout       0.80      0.73      0.77       284\n",
      "    Enrolled       0.53      0.30      0.38       151\n",
      "    Graduate       0.78      0.94      0.85       450\n",
      "\n",
      "    accuracy                           0.76       885\n",
      "   macro avg       0.70      0.66      0.67       885\n",
      "weighted avg       0.75      0.76      0.74       885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = dtree.predict(X_test)\n",
    "print(\"Decision Tree Accuracy\\t\\t :\",round(accuracy_score(y_test,y_pred)*100,5),\"%\")\n",
    "print(classification_report(y_test,y_pred,target_names=['Dropout','Enrolled','Graduate']))\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(\"RandomForest Accuracy\\t\\t :\",round(accuracy_score(y_test,y_pred)*100,5),\"%\")\n",
    "print(classification_report(y_test,y_pred,target_names=['Dropout','Enrolled','Graduate']))\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"LogisticRegression Accuracy\\t :\",round(accuracy_score(y_test,y_pred)*100,5),\"%\")\n",
    "print(classification_report(y_test,y_pred,target_names=['Dropout','Enrolled','Graduate']))\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(\"KNeighbors Accuracy\\t\\t :\",round(accuracy_score(y_test,y_pred)*100,5),\"%\")\n",
    "print(classification_report(y_test,y_pred,target_names=['Dropout','Enrolled','Graduate']))\n",
    "\n",
    "y_pred = abc.predict(X_test)\n",
    "print(\"AdaBoost Accuracy\\t\\t :\",round(accuracy_score(y_test,y_pred)*100,5),\"%\")\n",
    "print(classification_report(y_test,y_pred,target_names=['Dropout','Enrolled','Graduate']))\n",
    "\n",
    "y_pred = xbc.predict(X_test)\n",
    "print(\"XGBoost Accuracy\\t\\t :\",round(accuracy_score(y_test,y_pred)*100,5),\"%\")\n",
    "print(classification_report(y_test,y_pred,target_names=['Dropout','Enrolled','Graduate']))\n",
    "\n",
    "y_pred = svc.predict(X_test)\n",
    "print(\"SVM Accuracy\\t\\t\\t :\",round(accuracy_score(y_test,y_pred)*100,5),\"%\")\n",
    "print(classification_report(y_test,y_pred,target_names=['Dropout','Enrolled','Graduate'],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LALA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Voting Classifier Accuracy : 77.97 %\n",
      "{'0': {'precision': 0.7970479704797048, 'recall': 0.7605633802816901, 'f1-score': 0.7783783783783784, 'support': 284.0}, '1': {'precision': 0.5166666666666667, 'recall': 0.4105960264900662, 'f1-score': 0.4575645756457565, 'support': 151.0}, '2': {'precision': 0.8340080971659919, 'recall': 0.9155555555555556, 'f1-score': 0.8728813559322034, 'support': 450.0}, 'accuracy': 0.7796610169491526, 'macro avg': {'precision': 0.7159075781041211, 'recall': 0.6955716541091039, 'f1-score': 0.7029414366521127, 'support': 885.0}, 'weighted avg': {'precision': 0.7680021853193212, 'recall': 0.7796610169491526, 'f1-score': 0.7716930175722715, 'support': 885.0}}\n"
     ]
    }
   ],
   "source": [
    "ens1 = VotingClassifier(estimators=[('rfc', rfc), ('xbc',xbc)], voting='soft')\n",
    "ens1.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ens1.predict(X_test)\n",
    "print(\"Voting Classifier Accuracy :\",round(accuracy_score(y_test,y_pred)*100,2),\"%\")\n",
    "print(classification_report(y_test,y_pred,output_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LALA\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Voting Classifier Accuracy : 77.85 %\n"
     ]
    }
   ],
   "source": [
    "ens2 = VotingClassifier(estimators=[('rfc', rfc), ('lr', svc), ('abc',abc), ('xbc',xbc)], voting='soft')\n",
    "ens2.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ens2.predict(X_test)\n",
    "print(\"Voting Classifier Accuracy :\",round(accuracy_score(y_test,y_pred)*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
